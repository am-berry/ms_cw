{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Device-Checks\" data-toc-modified-id=\"Device-Checks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Device Checks</a></span></li><li><span><a href=\"#Environment-Set-up\" data-toc-modified-id=\"Environment-Set-up-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Environment Set-up</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing-States\" data-toc-modified-id=\"Preprocessing-States-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Preprocessing States</a></span></li></ul></li><li><span><a href=\"#Experience-Replay\" data-toc-modified-id=\"Experience-Replay-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Experience Replay</a></span></li><li><span><a href=\"#Double-Deep-Q-Learning-Agent\" data-toc-modified-id=\"Double-Deep-Q-Learning-Agent-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Double Deep-Q-Learning Agent</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "## CHECK AVAILABLE GPUs\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15555702220433769298\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12250420119185188348\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6515012665\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9336251322988050528\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10187297176570147243\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "## PRINT DEVICE DETAILS\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT LIBRARIES\n",
    "import gym\n",
    "from gym import Wrapper\n",
    "from gym import spaces\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape is (210, 160, 3)\n",
      "\n",
      "Possible actions are 9: \n",
      "['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n"
     ]
    }
   ],
   "source": [
    "## INITIATE THE ENVIRONMENT\n",
    "env = gym.make(\"MsPacmanDeterministic-v4\")\n",
    "\n",
    "# Print environment details\n",
    "print('State shape is {}\\n'.format(env.observation_space.shape))\n",
    "print('Possible actions are {}: '.format(env.action_space.n))\n",
    "print(env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE A FUNCTION TO PREPROCESS STATES\n",
    "def preprocess_state(state):\n",
    "    \"\"\"Reshape and transform an RGB state to grayscale\"\"\"\n",
    "    # Crop unecessary lives & points pannel and resize the image\n",
    "    state = state[1:176:2, ::2]\n",
    "    # Convert the image to greyscale\n",
    "    state = state.mean(axis=2)\n",
    "    # Normalize the image from -1 to +1\n",
    "    state = (state - 128) / 128-1\n",
    "    return state.reshape(88,80)\n",
    "\n",
    "## CREATE A CLASS THAT MODIFIES THE MsPacmanDeterministic-v4 GYM ENVIRONMENT\n",
    "class MsPacman(object):\n",
    "    \"\"\"Define environment parameters\"\"\"\n",
    "    def __init__(self, env, stack=4):\n",
    "        self.env = gym.make(env)\n",
    "        self.stack = stack\n",
    "        self.action_space = self.env.action_space\n",
    "        self.actions = range(self.env.action_space.n)\n",
    "        # Screen buffer of size 4 to be able to build state arrays of size [1, 4, 88, 80]\n",
    "        self.state_buffer = deque()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reshape a state into (88,80,1) and append state to the buffer\"\"\"\n",
    "        # Clear the state buffer\n",
    "        self.state_buffer = deque()\n",
    "        state = self.env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        stacked_state = np.stack([state for i in range(self.stack)], axis=0)\n",
    "\n",
    "        for i in range(self.stack-1):\n",
    "            self.state_buffer.append(state)\n",
    "        return stacked_state\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        \"\"\"Concatenate 3 previous states and current state to produce \n",
    "           a stacked state of shape (4,88,80)\"\"\"\n",
    "        state, reward, done, info = self.env.step(self.actions[action_index])\n",
    "        state = preprocess_state(state)\n",
    "        \n",
    "        \n",
    "\n",
    "        previous_frames = np.array(self.state_buffer)\n",
    "        stacked_state = np.empty((self.stack, 88, 80))\n",
    "        stacked_state[:self.stack-1, :] = previous_frames\n",
    "        stacked_state[self.stack-1] = state\n",
    "\n",
    "        # Pop the oldest frame and add the current frame to the queue\n",
    "        self.state_buffer.popleft()\n",
    "        self.state_buffer.append(state)\n",
    "\n",
    "        return stacked_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape is (4, 88, 80)\n",
      "\n",
      "Possible actions are 9: \n",
      "['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n"
     ]
    }
   ],
   "source": [
    "env = MsPacman(\"MsPacmanDeterministic-v4\",stack=4)\n",
    "print('State shape is {}\\n'.format(np.shape(env.reset())))\n",
    "print('Possible actions are {}: '.format(env.action_space.n))\n",
    "print(env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE A CLASS THAT STORES THE AGENT'S EXPERIENCES AND RETRIEVES RANDOM BATCHES FROM ITS MEMORY\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity=10000, batch_size=32):\n",
    "        \"\"\"Define the class parameters\"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.experience = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to the memory buffer\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)        \n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Sample a random batch of experiences from the memory\"\"\"\n",
    "        samples = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "        states = np.array([state[0] for state in samples])\n",
    "        actions = np.array([action[1] for action in samples])\n",
    "        rewards = np.array([reward[2] for reward in samples])\n",
    "        next_states = np.array([next_state[3] for next_state in samples])\n",
    "        dones = np.array([done[4] for done in samples])\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep-Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE THE DOUBLE DEEP-Q-LEARNING AGENT\n",
    "class DDQNAgent:\n",
    "\n",
    "    def __init__(self, action_space, capacity=10000, batch_size=32, \n",
    "                 learning_rate=0.0001, gamma = 0.99, epsilon=1.0, epsilon_min = 0.1, \n",
    "                 epsilon_decay = 0.999):\n",
    "        \"\"\"Create an instance of a Deep-Q-Network Agent\"\"\"\n",
    "        self.action_size = action_space.n # number of possible actions\n",
    "        self.learning_rate = learning_rate # model's learning rate\n",
    "        self.gamma = gamma # discount rate\n",
    "        self.batch_size = batch_size # size of batches sampled from memory\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = epsilon_min # threshold of epsilon\n",
    "        self.epsilon_decay = epsilon_decay # decay of exploration rate\n",
    "        self.model = self.build_model() # instance of dqn model\n",
    "        self.target_model = self.build_model() # target model\n",
    "        self.memory = ReplayMemory(capacity=capacity, batch_size=self.batch_size) # expirience replay memory\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build the Convolutional DQN model\"\"\"\n",
    "        model = Sequential()\n",
    "        # add convolutional layers\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(4,88,80), data_format='channels_first')) # provide as input shape the state shape\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        # add fully connected layers with dropout\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.action_size, activation='linear')) # provide as output the number of possible states\n",
    "        # compile model with Huber loss and RMSprop optimizer\n",
    "        model.compile(loss=Huber(), optimizer=RMSprop(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Select action according to the epsilon-greedy policy\"\"\"\n",
    "        # get the correct dimension that the models accepts\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        action = self.model.predict(state)\n",
    "        if random.random() > self.epsilon:\n",
    "            return np.argmax(action) # exploitation\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size)) # exploration\n",
    "\n",
    "    def train_model(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Train the model using random batches of experiences from the memory\"\"\"\n",
    "        \n",
    "        # start sampling from experience when memory is filled\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            states, actions, rewards, next_states, dones = self.memory.sample() # random memory sample\n",
    "            # iterate over the batch to fix dimensions\n",
    "\n",
    "            for i in range(len(states)):\n",
    "                state, action, reward, next_state, done = states[i], actions[i], rewards[i], next_states[i], dones[i]\n",
    "                state = np.expand_dims(state, axis=0)\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "            \n",
    "            targets = self.model.predict(states)\n",
    "            target_next = self.model.predict(next_states) #DQN\n",
    "            target_vals = self.target_model.predict(next_states) #Target model\n",
    "\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    # select an action using DQN model \n",
    "                    # update using the target model\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    targets[i][actions[i]] = (rewards[i] + self.gamma * (target_vals[i][a]))\n",
    "            \n",
    "            # fit to get get the loss of the model\n",
    "            loss = self.model.fit(states, targets, epochs=1, verbose=0).history['loss'][0]\n",
    "            return loss\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copy weights from the model used for action selection to the model used for computing targets\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def save_model(self):\n",
    "        \"\"\"Save the DQN Network\"\"\"\n",
    "        self.model.save('local_model.h5')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A RANDOM AGENT \n",
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        \"\"\"Create an instance of a random agent\"\"\"\n",
    "        self.action_size = action_space.n\n",
    "        \n",
    "    def get_action(self):\n",
    "        \"\"\"Select action uniformly at random\"\"\"\n",
    "        action = random.choice(np.arange(self.action_size))\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE TRAINING PARAMETERS\n",
    "EPISODES = 500 # Number of episodes\n",
    "cumulative_rewards = []\n",
    "\n",
    "# Get an instance of a DDQN agent\n",
    "ddqnAgent = DDQNAgent(env.action_space, capacity=10000, batch_size=32, \n",
    "                 learning_rate=0.001, gamma = 0.99, epsilon=1.0, epsilon_min = 0.1, epsilon_decay = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training initialised...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TRAIN THE DDQN AGENT\n",
    "print('Training initialised...\\n')\n",
    "for episode in range(1, EPISODES+1):\n",
    "    init_time = time()\n",
    "    state = env.reset()\n",
    "    C = 0.1*EPISODES # target update step\n",
    "    ep_reward = 0\n",
    "    lives = 3\n",
    "    done=False\n",
    "    dead=False\n",
    "    while not done:\n",
    "\n",
    "        action = ddqnAgent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = reward if not dead else -200  # penalise death        \n",
    "        ddqnAgent.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "        if ddqnAgent.epsilon >= ddqnAgent.epsilon_min: \n",
    "            ddqnAgent.epsilon *= ddqnAgent.epsilon_decay # decaying exploration/exploitation\n",
    "\n",
    "        loss = ddqnAgent.train_model(action, state, next_state, reward, done)\n",
    "\n",
    "        ep_reward += reward\n",
    "        state = next_state\n",
    "        dead = info['ale.lives'] < lives\n",
    "        lives = info['ale.lives']\n",
    "    \n",
    "    time_taken = time() - init_time # time taken to complete one episode\n",
    "    \n",
    "    if episode % C == 0:\n",
    "        ddqnAgent.update_target_model()\n",
    "    \n",
    "    cumulative_rewards.append(ep_reward)\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        print('Episode {} of {}:\\n   score: {:.1f} \\n   loss: {} \\n   \\u03B5: {:.2f} \\n   time taken: {:.3f} sec'.format(episode, EPISODES, ep_reward, loss, ddqnAgent.epsilon, time_taken))\n",
    "        ddqnAgent.save_model()\n",
    "        \n",
    "print('\\nTraining ended.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TIME-PROGRESSION INDICES\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Cumulative Reward per episode\n",
    "plt.plot(cumulative_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward per Episode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVALUATE THE TRAINED DDQN AGENT AGAINST A RANDOM AGENT\n",
    "saved_model = 'local_model.h5'\n",
    "DDQN_score = []\n",
    "test_agent = load_model(saved_model)\n",
    "RAND_score = []\n",
    "RAND_agent = RandomAgent(env.action_space)\n",
    "runs = 1\n",
    "\n",
    "# Run the trained agent for one episode\n",
    "for run in range(1,runs+1):\n",
    "    state = env.reset()\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = test_agent.predict(state)\n",
    "        env.render()\n",
    "        next_state, reward, done, info = env.step(np.argmax(action))\n",
    "        ep_reward += reward\n",
    "        state = next_state\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "    DDQN_score.append(ep_reward)    \n",
    "    env.close()\n",
    "\n",
    "# Run the random agent for one episode\n",
    "for run in range(1,runs+1):\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = RAND_agent.get_action()\n",
    "        env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        state = next_state\n",
    "    RAND_score.append(ep_reward)    \n",
    "    env.close()\n",
    "\n",
    "print('Trained agent score: {}, Random agent score: {}'.format(DDQN_score,RAND_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
